{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь только импорт\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import math\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as web\n",
    "import copy\n",
    "import progressbar\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process\n",
    "from joblib import Parallel, delayed, wrap_non_picklable_objects\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# примитивная функция для обрубания \"слишком\" больших значений\n",
    "# идёт от маленьких к большим, начиная с 15го (значение эмпирическое)\n",
    "# если следующее значение в 1.25 раза больше, то меняем на меньшее\n",
    "# если функцию не применять, то выбросы будут мешать другим значениям\n",
    "\n",
    "def cut_over(series_in):\n",
    "    New_series = copy.deepcopy(series_in)\n",
    "    New_series.sort_values(inplace = True, ascending = False)\n",
    "    for k in reversed(range(300)):\n",
    "        if 1.2 * New_series.values[k+1] < New_series.values[k]:\n",
    "            New_series.values[k] = New_series.values[k+1]\n",
    "    return New_series.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тот же cut_over, только для меньшего числа значений\n",
    "\n",
    "def cut_over_less(series_in):\n",
    "    New_series = copy.deepcopy(series_in)\n",
    "    New_series.sort_values(inplace = True, ascending = False)\n",
    "    for k in reversed(range(30)):\n",
    "        if 1.2 * New_series.values[k+1] < New_series.values[k]:\n",
    "            New_series.values[k] = New_series.values[k+1]\n",
    "    return New_series.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция аналогчна cut_over, только обрубает наименькие значения\n",
    "\n",
    "def cut_under(series_in):\n",
    "    New_series = copy.deepcopy(series_in)\n",
    "    New_series.sort_values(inplace = True, ascending = False)\n",
    "    for k in range(len(series_in)-300, len(series_in)-1):\n",
    "        if abs(New_series.values[k+1]) > 1.2 * abs(New_series.values[k]):\n",
    "            New_series.values[k+1] = New_series.values[k]\n",
    "    return New_series.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для расчёта медианы + 3 стандартных отклонения, понадобился для нормализации датасета\n",
    "def max_value(series_in):\n",
    "    return np.median(series_in) + 3 * np.std(series_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем список тикеров, торгуемых на СПб бирже\n",
      "Собрано 1268 тикеров\n",
      "Преобразуем список тикеров\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf6b0b932fc4e4f88a6eab74d57a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1268), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# формируем датасет с сайта http://geolenta.com/. Содержит все тикеры, торгующиеся на спб бирже\n",
    "# датасет содержит столбец тикеров, столбец имён компаний, столбец отрасли и столбец сектора\n",
    "\n",
    "print 'Собираем список тикеров, торгуемых на СПб бирже'\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "ticker_to_compare = ''\n",
    "i = 1\n",
    "while True:\n",
    "    table=pd.read_html('http://geolenta.com/?page=' + str(i), header = 0)\n",
    "    rg_table = table[0]\n",
    "    dataset_new = rg_table[[rg_table.columns[1], rg_table.columns[2], rg_table.columns[3], rg_table.columns[4]]]\n",
    "    dataset_new = dataset_new.drop([0])\n",
    "    if i > 1:\n",
    "        if dataset_new[dataset_new.columns[0]][1] == ticker_to_compare:\n",
    "            break\n",
    "    ticker_to_compare = dataset_new[dataset_new.columns[0]][1]\n",
    "    dataset = pd.concat([dataset, dataset_new])\n",
    "    dataset = dataset.reset_index(drop = True)\n",
    "    i = i + 1\n",
    "    \n",
    "tickers_only = dataset[dataset.columns[0]]\n",
    "print 'Собрано ' + str(len(dataset)) + ' тикеров'\n",
    "\n",
    "\n",
    "print 'Преобразуем список тикеров'\n",
    "list_of_names = list()\n",
    "for i in tqdm_notebook(range(len(dataset))):\n",
    "    if dataset[dataset.columns[1]][i] <> dataset[dataset.columns[1]][i]:\n",
    "        dataset[dataset.columns[1]][i] = tickers_only[i]\n",
    "    my_str = dataset[dataset.columns[1]][i]\n",
    "    my_str = my_str.replace('M/I Homes, Inc.', 'm-i-homes')\n",
    "    my_str = my_str.replace(', Inc.', '')\n",
    "    my_str = my_str.replace(' Inc.', '')\n",
    "    my_str = my_str.replace(' Group', '')\n",
    "    my_str = my_str.replace(' Limited', '')\n",
    "    my_str = my_str.replace(' Corporation', '')\n",
    "    my_str = my_str.replace(' Company', '')\n",
    "    my_str = my_str.replace(' plc', '')\n",
    "    my_str = my_str.replace(' class A', '')\n",
    "    my_str = my_str.replace(' ClassA', '')\n",
    "    my_str = my_str.replace(' Class A', '')\n",
    "    my_str = my_str.replace(' class B', '')\n",
    "    my_str = my_str.replace(' ClassB', '')\n",
    "    my_str = my_str.replace(' Class B', '')\n",
    "    my_str = my_str.replace(' class C', '')\n",
    "    my_str = my_str.replace(' Class C', '')\n",
    "    my_str = my_str.replace(' ClassC', '')\n",
    "    my_str = my_str.replace('The ', '')\n",
    "    my_str = my_str.replace(' Incorporated', '')\n",
    "    my_str = my_str.replace(' & Co.', '')\n",
    "    my_str = my_str.replace(' ADR', '')\n",
    "    my_str = my_str.replace(' CORP.', '')\n",
    "    my_str = my_str.replace('.com', '')\n",
    "    my_str = my_str.replace('.', '')\n",
    "    my_str = my_str.replace('&', 'and')\n",
    "    my_str = my_str.replace(' InBev SA/NV', '')\n",
    "    my_str = my_str.replace('IAC/InterActiveCorp', 'iac-interactivecorp')\n",
    "    \n",
    "    my_str = my_str.replace(' ', '-')\n",
    "    my_str = my_str.replace(\"'\", '')\n",
    "    my_str = my_str.lower()\n",
    "    list_of_names.append(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_names = [x.encode('utf-8') for x in list_of_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_only = [x.encode('utf-8') for x in tickers_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем нулями DataFrame\n",
    "\n",
    "dataset['revenue_growth_10y'] = 0\n",
    "dataset['revenue_growth_5y'] = 0\n",
    "dataset['revenue_coef'] = 0\n",
    "dataset['eps_growth_10y'] = 0\n",
    "dataset['eps_growth_5y'] = 0\n",
    "dataset['next_5yr_growth'] = 0\n",
    "dataset['eps_coef'] = 0\n",
    "dataset['R_square_10y'] = 0\n",
    "dataset['years_profitable'] = 0\n",
    "dataset['years_up'] = 0\n",
    "dataset['years_strike'] = 0\n",
    "dataset['market_cap'] = 0\n",
    "dataset['buyback'] = 0\n",
    "dataset['ROE'] = 0\n",
    "dataset['mean_ROE'] = 0\n",
    "dataset['ROA'] = 0\n",
    "dataset['mean_ROA'] = 0\n",
    "dataset['Shareholder_Equity_growth'] = 0\n",
    "dataset['Profit_margin'] = 0\n",
    "dataset['Profit_margin_mean'] = 0\n",
    "dataset['Quick_ratio'] = 0\n",
    "dataset['Current_ratio'] = 0\n",
    "dataset['Debt_Equity'] = 0\n",
    "dataset['payout'] = 0\n",
    "dataset['payout_5yr'] = 0\n",
    "dataset['CAGR'] = 0\n",
    "dataset['max_dd'] = 0\n",
    "dataset['sharpe'] = 0\n",
    "dataset['sortino'] = 0\n",
    "dataset['TTM_div_yield'] = 0\n",
    "dataset['div_inc_years'] = 0\n",
    "dataset['div_growth_3yr'] = 0\n",
    "dataset['div_growth_5yr'] = 0\n",
    "dataset['div_yield_5yr'] = 0\n",
    "dataset['curr_stock_price'] = 0\n",
    "dataset['52_week_high'] = 0\n",
    "dataset['52_week_low'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок парсинга значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем дивидендную историю с www.dividend.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# парсим сколько лет подряд повышали дивы/ Годная инфа только на www.dividend.com, но если платят менее 10 лет подряд, то инфы нет\n",
    "# единственный параметр, который можно намайнить не в цикле\n",
    "\n",
    "print 'Собираем дивидендную историю с www.dividend.com'\n",
    "\n",
    "table=pd.read_html('https://www.dividend.com/dividend-stocks/10-year-dividend-increasing-stocks')\n",
    "rg_table = table[0]\n",
    "inc_years_table = rg_table[rg_table.columns[[1, 4]]]\n",
    "inc_years_table.rename(columns={'Stock Symbol': dataset.columns[0]}, inplace=True)\n",
    "result = pd.merge(dataset, inc_years_table, how=\"left\", on=dataset.columns[0])\n",
    "dataset['div_inc_years'] = result['No. of Years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_web_data(i):\n",
    "\n",
    "    # в стандартном именовании тикеров присутствует дефис перед классом акции\n",
    "    # сайт macrotrends ест тикеры с точкой вместо дефиса\n",
    "    \n",
    "    tickers_only[i] = tickers_only[i].replace('-', '.')\n",
    "\n",
    "    # набиваем revenue_growth\n",
    "\n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/revenue')\n",
    "        rg_table = table[0]\n",
    "        rg_table = rg_table.iloc[:, 1].values\n",
    "        rg_table = [s.replace('$', '') for s in rg_table]\n",
    "        rg_table = [s.replace(',', '') for s in rg_table]\n",
    "    except:\n",
    "        dataset.loc[i, 'revenue_growth_5y'] = None \n",
    "        dataset.loc[i, 'revenue_growth_10y'] = None\n",
    "    \n",
    "    try:\n",
    "        x = map(float, rg_table[:rg_table.index('')]) \n",
    "        x_rev = np.flip(x)\n",
    "    except:\n",
    "        try:\n",
    "            x = map(float, rg_table) \n",
    "            x_rev = np.flip(x)\n",
    "        except:\n",
    "            x = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        revenue_growth_5y = round(((float(rg_table[0]) / float(rg_table[5])) ** (1./5.) - 1)*100, 2)\n",
    "        dataset.loc[i, 'revenue_growth_5y'] = revenue_growth_5y\n",
    "    except:\n",
    "        dataset.loc[i, 'revenue_growth_5y'] = None        \n",
    "\n",
    "    try:\n",
    "        revenue_growth = round(((float(rg_table[0]) / float(rg_table[10])) ** (1./10.) - 1)*100, 2)\n",
    "        dataset.loc[i, 'revenue_growth_10y'] = revenue_growth\n",
    "    except:\n",
    "        try:\n",
    "            revenue_growth = round(((float(rg_table[0]) / float(rg_table[len(rg_table)-1])) ** (1./(len(rg_table)-1)) - 1)*100, 2)\n",
    "            dataset.loc[i, 'revenue_growth_10y'] = revenue_growth\n",
    "        except:\n",
    "            dataset.loc[i, 'revenue_growth_10y'] = None\n",
    "\n",
    "    try:\n",
    "        revenue_change = np.zeros(9)\n",
    "        for k in range (len(x_rev)-10, len(x_rev)-1):\n",
    "            revenue_change[k] = (x_rev[k+1] - x_rev[k])/(x_rev[k])*100.\n",
    "        revenue_coef = round(np.mean(revenue_change) / np.std(revenue_change), 2) \n",
    "        dataset.loc[i, 'revenue_coef'] = revenue_coef\n",
    "    except:\n",
    "        try:\n",
    "            revenue_change = np.zeros(len(x_rev)-1)\n",
    "            for k in range (0, len(x_rev)-1):\n",
    "                revenue_change[k] = (x_rev[k+1] - x_rev[k])/(x_rev[k])*100.\n",
    "            revenue_coef = round(np.mean(revenue_change) / np.std(revenue_change), 2) \n",
    "            dataset.loc[i, 'revenue_coef'] = revenue_coef\n",
    "        except:\n",
    "            dataset.loc[i, 'revenue_coef'] = None\n",
    "\n",
    "    # набиваем прогнозный eps_growth\n",
    "    try:\n",
    "        table=pd.read_html('https://finance.yahoo.com/quote/' + tickers_only[i] +'/analysis?p=' + tickers_only[i])\n",
    "        rg_table = table[5]\n",
    "\n",
    "        next_5yr_growth = rg_table[tickers_only[i]][4]\n",
    "        str_finish = string.find(next_5yr_growth, '%')\n",
    "        next_5yr_growth = float(next_5yr_growth[:str_finish])\n",
    "    except:\n",
    "        next_5yr_growth = 0\n",
    "    dataset.loc[i, 'next_5yr_growth'] = next_5yr_growth\n",
    "\n",
    "    # набиваем eps_growth\n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/eps-earnings-per-share-diluted')\n",
    "        rg_table = table[0]\n",
    "        rg_table = rg_table.iloc[:, 1].values\n",
    "        rg_table = [s.replace('$', '') for s in rg_table]\n",
    "        rg_table = [s.replace(',', '') for s in rg_table]\n",
    "    except:\n",
    "        dataset.loc[i, 'eps_growth_5y'] = None\n",
    "    \n",
    "    try:\n",
    "        y = rg_table[:rg_table.index('0.00')]\n",
    "        y = map(float, y) \n",
    "    except:\n",
    "        try:\n",
    "            y = rg_table\n",
    "            y = map(float, y) \n",
    "        except:\n",
    "            y = list()\n",
    "    \n",
    "\n",
    "    try:\n",
    "        y_10 = y[:11]\n",
    "        while y_10[len(y_10)-1] == 0:\n",
    "            del y[len(y_10)-1]\n",
    "        X = np.linspace(1, len(y_10), num=len(y_10))\n",
    "        X = X.reshape(-1, 1)\n",
    "    except:\n",
    "        y_10 = y\n",
    "\n",
    "    try:\n",
    "        reg = LinearRegression().fit(X, y_10)\n",
    "        R_square = round(reg.score(X, y_10), 2)\n",
    "        dataset.loc[i, 'R_square_10y'] = R_square\n",
    "    except:\n",
    "        dataset.loc[i, 'R_square_10y'] = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_rev = np.flip(y)\n",
    "\n",
    "    try:\n",
    "        eps_change = np.zeros(9)\n",
    "        for k in range (len(y_rev)-10, len(y_rev)-1):\n",
    "            eps_change[k] = (y_rev[k+1] - y_rev[k])/(y_rev[k])*100.\n",
    "        eps_coef = round(np.mean(eps_change) / np.std(eps_change), 2) \n",
    "        dataset.loc[i, 'eps_coef'] = eps_coef\n",
    "    except:\n",
    "        try:\n",
    "            eps_change = np.zeros(len(y_rev)-1)\n",
    "            for k in range (0, len(y_rev)-1):\n",
    "                eps_change[k] = (y_rev[k+1] - y_rev[k])/(y_rev[k])*100.\n",
    "            eps_coef = round(np.mean(eps_change) / np.std(eps_change), 2) \n",
    "            dataset.loc[i, 'eps_coef'] = eps_coef\n",
    "        except:\n",
    "            dataset.loc[i, 'eps_coef'] = None\n",
    "\n",
    "    years_profitable = 0\n",
    "    years_up = 0\n",
    "    years_strike = 0\n",
    "    for k in range(len(y)):\n",
    "        if y[k] >= 0:\n",
    "            years_profitable += 1\n",
    "    for k in range(len(y) - 1):\n",
    "        if y[k] > y[k+1]:\n",
    "            years_up += 1\n",
    "    for k in range(len(y) - 1):\n",
    "        if y[k] > y[k+1]:\n",
    "            years_strike += 1    \n",
    "        else:\n",
    "            break\n",
    "    try:\n",
    "        years_profitable_rel = float(years_profitable) \n",
    "        dataset.loc[i, 'years_profitable'] = years_profitable_rel\n",
    "    except:\n",
    "        dataset.loc[i, 'years_profitable'] = None\n",
    "\n",
    "    try:\n",
    "        years_up_rel = float(years_up)\n",
    "        dataset.loc[i, 'years_up'] = years_up_rel\n",
    "    except:\n",
    "        dataset.loc[i, 'years_up'] = None\n",
    "\n",
    "    try:\n",
    "        years_strike_rel = float(years_strike)\n",
    "        dataset.loc[i, 'years_strike'] = years_strike_rel\n",
    "    except:\n",
    "        dataset.loc[i, 'years_strike'] = None    \n",
    "\n",
    "    try:\n",
    "        eps_growth_5y = round((((y[0] / y[5]) ** (1./5.)) - 1)*100, 2)\n",
    "        dataset.loc[i, 'eps_growth_5y'] = eps_growth_5y\n",
    "    except:\n",
    "        try:\n",
    "            eps_growth_5y = round(((y[0] / y[6]) ** (1./(6.)) - 1)*100, 2)\n",
    "            dataset.loc[i, 'eps_growth_5y'] = eps_growth_5y\n",
    "        except:\n",
    "            try:\n",
    "                eps_growth_10y = round(((y[0] / y[4]) ** (1./(4.)) - 1)*100, 2)\n",
    "                dataset.loc[i, 'eps_growth_5y'] = eps_growth_5y\n",
    "            except:\n",
    "                dataset.loc[i, 'eps_growth_5y'] = None\n",
    "\n",
    "    try:\n",
    "        eps_growth_10y = round(((y[0] / y[10]) ** (1./(10.)) - 1)*100, 2)\n",
    "        dataset.loc[i, 'eps_growth_10y'] = eps_growth_10y\n",
    "    except:\n",
    "        try:\n",
    "            eps_growth_10y = round(((y[0] / y[11]) ** (1./(11.)) - 1)*100, 2)\n",
    "            dataset.loc[i, 'eps_growth_10y'] = eps_growth_10y\n",
    "        except:\n",
    "            try:\n",
    "                eps_growth_10y = round(((y[0] / y[9]) ** (1./(9.)) - 1)*100, 2)\n",
    "                dataset.loc[i, 'eps_growth_10y'] = eps_growth_10y\n",
    "            except:\n",
    "                dataset.loc[i, 'eps_growth_10y'] = None\n",
    "\n",
    "    # набиваем market cap    \n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/market-cap')\n",
    "        market_cap = table[0].iloc[:, 2].values[0]\n",
    "        market_cap = market_cap.replace('$', '')\n",
    "        market_cap = float(market_cap.replace('B', ''))\n",
    "        dataset.loc[i, 'market_cap'] = market_cap\n",
    "    except:\n",
    "        dataset.loc[i, 'market_cap'] = None\n",
    "\n",
    "    # набиваем shares_outstanding_reduction (buyback)\n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/shares-outstanding')\n",
    "        rg_table = table[0]\n",
    "        rg_table = rg_table.iloc[:, 1].values\n",
    "        rg_table = map(float, rg_table)\n",
    "        k = 2\n",
    "        buyback = round(100 * (rg_table[5] - rg_table[0]) / rg_table[5], 2)\n",
    "        while math.isnan(buyback):\n",
    "            buyback = round(100 * (rg_table[5-k] - rg_table[0]) / rg_table[5-k], 2)\n",
    "            k += 1\n",
    "        dataset.loc[i, 'buyback'] = buyback\n",
    "    except:\n",
    "        dataset.loc[i, 'buyback'] = None\n",
    "\n",
    "\n",
    "    # набиваем ROE, среднее ROE и Shareholder's Equity    \n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/roe')\n",
    "        rg_table = table[0]\n",
    "        ROE = rg_table.iloc[:, 3].values\n",
    "        ROE = [s.replace('%', '') for s in ROE]\n",
    "        ROE = map(float, ROE)\n",
    "        ROE_now = ROE[0]\n",
    "        dataset.loc[i, 'ROE'] = ROE_now\n",
    "\n",
    "        '''\n",
    "        j = 0\n",
    "        ROE_new = list()\n",
    "        for j in range(len(ROE)):\n",
    "            if abs(ROE[j]) < 3 * abs(np.mean(ROE)):\n",
    "                ROE_new.append(ROE[j])\n",
    "        mean_ROE = round(np.mean(ROE_new), 2)\n",
    "        '''\n",
    "        mean_ROE = round(np.median(ROE), 2)\n",
    "        dataset.loc[i, 'mean_ROE'] = mean_ROE\n",
    "\n",
    "        Shareholder_Equity = rg_table.iloc[:, 2].values\n",
    "        Shareholder_Equity = filter(lambda v: v==v, Shareholder_Equity)\n",
    "        Shareholder_Equity = [s.replace('$', '') for s in Shareholder_Equity]\n",
    "        Shareholder_Equity = [s.replace('B', '') for s in Shareholder_Equity]\n",
    "        Shareholder_Equity = map(float, Shareholder_Equity)\n",
    "        try:\n",
    "            Shareholder_Equity_growth = round(np.mean(Shareholder_Equity) / np.std(Shareholder_Equity), 2)\n",
    "        except:\n",
    "            Shareholder_Equity_growth = 0\n",
    "        dataset.loc[i, 'Shareholder_Equity_growth'] = Shareholder_Equity_growth\n",
    "    except:\n",
    "        dataset.loc[i, 'ROE'] =  None\n",
    "        dataset.loc[i, 'mean_ROE'] =  None\n",
    "        dataset.loc[i, 'Shareholder_Equity_growth'] =  None\n",
    "\n",
    "  # набиваем ROA, среднее ROA\n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/roa')\n",
    "        rg_table = table[0]\n",
    "        ROA = rg_table.iloc[:, 3].values\n",
    "        ROA = [s.replace('%', '') for s in ROA]\n",
    "        ROA = map(float, ROA)\n",
    "        ROA_now = ROA[0]\n",
    "        dataset.loc[i, 'ROA'] = ROA_now\n",
    "\n",
    "        mean_ROA = round(np.median(ROA), 2)\n",
    "        dataset.loc[i, 'mean_ROA'] = mean_ROA\n",
    "\n",
    "    except:\n",
    "        dataset.loc[i, 'ROA'] =  None\n",
    "        dataset.loc[i, 'mean_ROA'] =  None\n",
    "\n",
    "    # набиваем net-profit-margin  \n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/net-profit-margin')\n",
    "        rg_table = table[0]\n",
    "        Profit_margin = rg_table.iloc[:, 3].values\n",
    "        Profit_margin = [s.replace('%', '') for s in Profit_margin]\n",
    "        Profit_margin = map(float, Profit_margin)\n",
    "        Profit_margin_now = Profit_margin[0]\n",
    "        dataset.loc[i, 'Profit_margin'] = Profit_margin_now\n",
    "        '''\n",
    "        try:\n",
    "            mean_Profit_margin = round(np.mean(Profit_margin[:10]), 2)\n",
    "        except:\n",
    "            mean_Profit_margin = round(np.mean(Profit_margin), 2)\n",
    "        '''\n",
    "        mean_Profit_margin = round(np.median(Profit_margin), 2)\n",
    "        dataset.loc[i, 'Profit_margin_mean'] = mean_Profit_margin\n",
    "    except:\n",
    "        dataset.loc[i, 'Profit_margin'] =  None\n",
    "        dataset.loc[i, 'Profit_margin_mean'] =  None\n",
    "\n",
    "\n",
    "    # набиваем Quick и Current ratios    \n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/current-ratio')\n",
    "        rg_table = table[0]\n",
    "        trigger = rg_table.iloc[:, 2].values[0]\n",
    "        Current_ratio = map(float, rg_table.iloc[:, 3].values)\n",
    "        try:\n",
    "            trigger = trigger.replace('$', '')\n",
    "            trigger = float(trigger.replace('B', ''))\n",
    "            Current_ratio_now = Current_ratio[0]\n",
    "        except:\n",
    "            Current_ratio_now = Current_ratio[1]\n",
    "\n",
    "        dataset.loc[i, 'Current_ratio'] = Current_ratio_now\n",
    "\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/quick-ratio')\n",
    "        rg_table = table[0]\n",
    "        trigger = rg_table.iloc[:, 2].values[0]\n",
    "        Quick_ratio = map(float, rg_table.iloc[:, 3].values)\n",
    "        try:\n",
    "            trigger = trigger.replace('$', '')\n",
    "            trigger = float(trigger.replace('B', ''))\n",
    "            Quick_ratio_now = Quick_ratio[0]\n",
    "        except:\n",
    "            Quick_ratio_now = Quick_ratio[1]\n",
    "\n",
    "        dataset.loc[i, 'Quick_ratio'] = Quick_ratio_now\n",
    "    except:\n",
    "        dataset.loc[i, 'Current_ratio'] =  None\n",
    "        dataset.loc[i, 'Quick_ratio'] =  None\n",
    "\n",
    "\n",
    "    # набиваем Debt_Equity    \n",
    "    try:\n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i] + '/debt-equity-ratio')\n",
    "        rg_table = table[0]\n",
    "        trigger = rg_table.iloc[:, 2].values[0]\n",
    "        Debt_Equity = map(float, rg_table.iloc[:, 3].values)\n",
    "        try:\n",
    "            trigger = trigger.replace('$', '')\n",
    "            trigger = float(trigger.replace('B', ''))\n",
    "            Debt_Equity_now = Debt_Equity[0]\n",
    "        except:\n",
    "            Debt_Equity_now = Debt_Equity[1]\n",
    "        dataset.loc[i, 'Debt_Equity'] = Debt_Equity_now\n",
    "    except:\n",
    "        dataset.loc[i, 'Debt_Equity'] =  None\n",
    "\n",
    "\n",
    "    # для yahoo finance меняем обратно точку на дефис\n",
    "    tickers_only[i] = tickers_only[i].replace('.', '-')\n",
    "\n",
    "\n",
    "    # набиваем payout    \n",
    "    try:\n",
    "        table=pd.read_html('https://finance.yahoo.com/quote/' + tickers_only[i])\n",
    "        dataset.loc[i, 'payout'] = round(100 * float(table[1][1][5][:5]) / float(table[1][1][3]), 2)\n",
    "    except:\n",
    "\n",
    "        dataset.loc[i, 'payout'] =  0\n",
    "\n",
    "    # набиваем CAGR, max_dd, sharpe, sortino. Начинаем брать после кризиса 2008, т.к. иначе будет не честно по отношению\n",
    "    # к компаниям, которые в кризис существовали и претерпели просадку бОльшую, чем новые компании\n",
    "\n",
    "    try:\n",
    "        price_history = web.get_data_yahoo(tickers_only[i],'01/01/2009',interval='m')        \n",
    "        price_history = price_history['Adj Close'].values\n",
    "\n",
    "        num = len(price_history) / 12.\n",
    "        CAGR = (price_history[len(price_history)-1] / price_history[0]) ** (1. / num) - 1\n",
    "        CAGR = round(CAGR * 100, 2)\n",
    "\n",
    "        daily_change = np.zeros(len(price_history)-1)\n",
    "        max_drawdown_array = np.zeros((len(price_history)-1, len(price_history)-1))\n",
    "        for k in range (0, len(price_history)-1):\n",
    "            daily_change[k] = (price_history[k+1] - price_history[k])/(price_history[k])*100.\n",
    "            for j in range (k, len(price_history)-1):\n",
    "                max_drawdown_array[k][j] = (price_history[k] - price_history[j])/(price_history[k])*100.\n",
    "        max_drawdown = round(max(map(max, max_drawdown_array)), 2)\n",
    "\n",
    "        sharpe_ratio = round((12**0.5) * np.mean(daily_change) / np.std(daily_change), 2) \n",
    "\n",
    "        daily_change_negative = filter(lambda x: x < 0, daily_change)\n",
    "        sortino_ratio = round((12**0.5) * np.mean(daily_change) / np.std(daily_change_negative), 2) \n",
    "\n",
    "        dataset.loc[i, 'CAGR'] = CAGR\n",
    "        dataset.loc[i, 'max_dd'] = max_drawdown\n",
    "        dataset.loc[i, 'sharpe'] = sharpe_ratio\n",
    "        dataset.loc[i, 'sortino'] = sortino_ratio\n",
    "    except:\n",
    "        dataset.loc[i, 'CAGR'] = None\n",
    "        dataset.loc[i, 'max_dd'] = None\n",
    "        dataset.loc[i, 'sharpe'] = None\n",
    "        dataset.loc[i, 'sortino'] = None\n",
    "\n",
    "    # парсим TTM dividend yield\n",
    "\n",
    "    table=pd.read_html('https://finance.yahoo.com/quote/' + tickers_only[i])\n",
    "    rg_table = table[1]\n",
    "    ttm_div_str = rg_table[1][5]\n",
    "    str_start = string.find(ttm_div_str, '(') + 1\n",
    "    str_finish = string.find(ttm_div_str, '%')\n",
    "    try:\n",
    "        ttm_div_yield = float(ttm_div_str[str_start:str_finish])\n",
    "    except:\n",
    "        ttm_div_yield = 0\n",
    "    dataset.loc[i, 'TTM_div_yield'] = ttm_div_yield\n",
    "\n",
    "    # для сайта dividendinvestor меняем дефис в тикерах на двойное нижнее подчёркивание\n",
    "    tickers_only[i] = tickers_only[i].replace('-', '___')\n",
    "    if tickers_only[i] == 'BRK___A':\n",
    "        tickers_only[i] = 'BRK__A'\n",
    "    if tickers_only[i] == 'BRK___B':\n",
    "        tickers_only[i] = 'BRK__B'   \n",
    "    if tickers_only[i] == 'LEN___B':\n",
    "        tickers_only[i] = 'LEN__B'     \n",
    "    if tickers_only[i] == 'RDS___A':\n",
    "        tickers_only[i] = 'rdsa'\n",
    "    if tickers_only[i] == 'RDS___B':\n",
    "        tickers_only[i] = 'rds__b'\n",
    "\n",
    "\n",
    "    try:    \n",
    "        page = urllib2.urlopen('https://www.dividendinvestor.com/dividend-quote/' + tickers_only[i])\n",
    "        soup = BeautifulSoup(page)\n",
    "        divs = str(soup)\n",
    "    except:\n",
    "        page = urllib2.urlopen('https://www.dividendinvestor.com/dividend-quote/' + tickers_only[i])\n",
    "        soup = BeautifulSoup(page)\n",
    "        divs = str(soup)\n",
    "\n",
    "    # парсим сколько лет подряд повышали дивы (если <10)\n",
    "    try:\n",
    "        str_start = string.find(divs, 'Consecutive Dividend Increases')+55\n",
    "        cons_years_str = divs[str_start:]\n",
    "        str_start = string.find(cons_years_str, '>')+1\n",
    "        str_finish = string.find(cons_years_str, 'Years') - 1\n",
    "        cons_years = float(cons_years_str[str_start:str_finish])\n",
    "        if dataset['div_inc_years'][i] <> dataset['div_inc_years'][i]:\n",
    "            if cons_years < 10:\n",
    "                dataset.loc[i, 'div_inc_years'] = cons_years\n",
    "            else:\n",
    "                dataset.loc[i, 'div_inc_years'] = 0\n",
    "    except:\n",
    "        dataset.loc[i, 'div_inc_years'] = 0\n",
    "\n",
    "      \n",
    "    # парсим 5 year average payout\n",
    "    try:\n",
    "        str_start = string.find(divs, 'Dividend Payout Ratio 5 yr Average')+60\n",
    "        str_finish = string.find(divs, '%', str_start)\n",
    "        payout_5yr = float(divs[str_start:str_finish].replace(',',''))\n",
    "        dataset.loc[i, 'payout_5yr'] = payout_5yr\n",
    "    except:\n",
    "        dataset.loc[i, 'payout_5yr'] = 0\n",
    "\n",
    "    # парсим 5 year dividend growth\n",
    "    try:\n",
    "        str_start = string.find(divs, 'Dividend Growth Rate 5 yr Average')+59\n",
    "        str_finish = string.find(divs, '%', str_start)\n",
    "        div_growth_5yr = float(divs[str_start:str_finish].replace(',',''))\n",
    "        dataset.loc[i, 'div_growth_5yr'] = div_growth_5yr\n",
    "    except:\n",
    "        dataset.loc[i, 'div_growth_5yr'] = 0\n",
    "\n",
    "    # парсим 3 year dividend growth\n",
    "    try:\n",
    "        str_start = string.find(divs, 'Dividend Growth Rate 3 yr Average')+59\n",
    "        str_finish = string.find(divs, '%', str_start)\n",
    "        div_growth_3yr = float(divs[str_start:str_finish].replace(',',''))\n",
    "        dataset.loc[i, 'div_growth_3yr'] = div_growth_3yr\n",
    "    except:\n",
    "        dataset.loc[i, 'div_growth_3yr'] = 0\n",
    "\n",
    "    # парсим Dividend Yield 5 Year Average\n",
    "    try:\n",
    "        str_start = string.find(divs, 'Dividend Yield 5 Year Average')+55\n",
    "        str_finish = string.find(divs, '%', str_start)\n",
    "        div_yield_5yr = float(divs[str_start:str_finish])\n",
    "        dataset.loc[i, 'div_yield_5yr'] = div_yield_5yr\n",
    "    except:\n",
    "        dataset.loc[i, 'div_yield_5yr'] = 0\n",
    "\n",
    "    # парсим Latest Close Stock Price\n",
    "    try:\n",
    "        str_start = string.find(divs, 'Latest Close Stock Price')+51\n",
    "        str_finish = string.find(divs, '<', str_start)\n",
    "        curr_stock_price = float(divs[str_start:str_finish].replace(',',''))\n",
    "        dataset.loc[i, 'curr_stock_price'] = curr_stock_price\n",
    "    except:\n",
    "        dataset.loc[i, 'curr_stock_price'] = 0\n",
    "\n",
    "    # парсим 52-Week High / Low Stock Price\n",
    "    try:\n",
    "        str_start = string.find(divs, '52-Week Low/High Stock Price')+10\n",
    "        high_price = divs[str_start:]\n",
    "        str_start = string.find(high_price, '-') + 3\n",
    "        high_price = high_price[str_start:]\n",
    "        str_finish = string.find(high_price, '<')\n",
    "        high_price = float(high_price[:str_finish].replace(',',''))\n",
    "        dataset.loc[i, '52_week_high'] = high_price\n",
    "    except:\n",
    "        dataset.loc[i, '52_week_high'] = 0\n",
    "\n",
    "    try:\n",
    "        str_start = string.find(divs, '52-Week Low/High Stock Price')+10\n",
    "        low_price = divs[str_start:]\n",
    "        str_start = string.find(low_price, '$') + 1\n",
    "        low_price = low_price[str_start:]\n",
    "        str_finish = string.find(low_price, '-') - 1\n",
    "        low_price = float(low_price[:str_finish].replace(',',''))\n",
    "        dataset.loc[i, '52_week_low'] = low_price\n",
    "    except:\n",
    "        dataset.loc[i, '52_week_low'] = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем фундаментальные показатели компаний из разных источников\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a403e5ed073d450092677287905ac79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1268), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 38min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "jobs_count = multiprocessing.cpu_count()\n",
    "\n",
    "print 'Собираем фундаментальные показатели компаний из разных источников'\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs = jobs_count):\n",
    "    Parallel()(delayed(parsing_web_data)(m) for m in tqdm_notebook(range(len(dataset))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок заполнения пропусков и ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulfil(i):\n",
    "    \n",
    "# заполним данные по сортино и просадке, которые считались с ошибкой, из другого API. Ошибка у нас при сортино>3 и max_dd>90\n",
    "\n",
    "    if dataset_ff['sortino'][i] > 3:\n",
    "        if dataset_ff['max_dd'][i] > 90:\n",
    "            tickers_for_ff = dataset_ff[dataset_ff.columns[0]].values[i]\n",
    "            try:\n",
    "                panel_data = data.DataReader(tickers_for_ff, 'yahoo', start_date, end_date)\n",
    "                price_history = panel_data['Adj Close'].values\n",
    "                daily_change = np.zeros(len(price_history)-1)\n",
    "                max_drawdown_array = np.zeros((len(price_history)-1, len(price_history)-1))\n",
    "                for k in range (0, len(price_history)-1):\n",
    "                    daily_change[k] = (price_history[k+1] - price_history[k])/(price_history[k])*100.\n",
    "                    for j in range (k, len(price_history)-1):\n",
    "                        max_drawdown_array[k][j] = (price_history[k] - price_history[j])/(price_history[k])*100.\n",
    "                max_drawdown = round(max(map(max, max_drawdown_array)), 2)\n",
    "                dataset_ff['max_dd'][i] = max_drawdown\n",
    "\n",
    "\n",
    "                daily_change_negative = filter(lambda x: x < 0, daily_change)\n",
    "                sortino_ratio = round((12**0.5) * np.mean(daily_change) / np.std(daily_change_negative), 2) \n",
    "                dataset_ff['sortino'][i] = sortino_ratio\n",
    "            except: \n",
    "                dataset_ff['max_dd'][i] = 100\n",
    "                dataset_ff['sortino'][i] = 10\n",
    "                \n",
    "# ошибка всё ещё может остаться, заменяем ошибочные сортино на минимум, max_dd на 100\n",
    "\n",
    "    if dataset_ff['sortino'][i] > 3:\n",
    "        if dataset_ff['max_dd'][i] > 90:\n",
    "            dataset_ff['sortino'][i] = np.min(dataset_ff['sortino'].values)\n",
    "            dataset_ff['max_dd'][i] = 100\n",
    "            \n",
    "    \n",
    "# меняем отрицательные ROE на нулевые, а если ROE по модулю превышает удвоенный средний ROE, то переписываем средний в текущий\n",
    "    if abs(dataset_ff['ROE'][i]) > 2 * abs(dataset_ff['mean_ROE'][i]):\n",
    "        dataset_ff['ROE'][i] = dataset_ff['mean_ROE'][i]\n",
    "    if dataset_ff['ROE'][i] < 0:\n",
    "        dataset_ff['ROE'][i] = 0\n",
    "# меняем отрицательные ROA на нулевые, а если ROA по модулю превышает удвоенный средний ROA, то переписываем средний в текущий\n",
    "    if abs(dataset_ff['ROA'][i]) > 2 * abs(dataset_ff['mean_ROA'][i]):\n",
    "        dataset_ff['ROA'][i] = dataset_ff['mean_ROA'][i]\n",
    "    if dataset_ff['ROA'][i] < 0:\n",
    "        dataset_ff['ROA'][i] = 0\n",
    "# меняем profit margin меньшие -100 на -100, а если profit margin по модулю превышает удвоенный средний profit margin, то переписываем средний в текущий\n",
    "    if abs(dataset_ff['Profit_margin'][i]) > 2 * abs(dataset_ff['Profit_margin_mean'][i]):\n",
    "        dataset_ff['Profit_margin'][i] = dataset_ff['Profit_margin_mean'][i]\n",
    "    if dataset_ff['Profit_margin'][i] < -100:\n",
    "        dataset_ff['Profit_margin'][i] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пора обработать пропуски в данных\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60c9119530d489e85d45047467ff8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Исправляем ошибки в данных\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8583e1a2d7954b17a465c2158f4db866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1268), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ошибки удалены, данные очищены\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print 'Пора обработать пропуски в данных'\n",
    "\n",
    "dataset_ff = copy.deepcopy(dataset)\n",
    "\n",
    "now = datetime.now()\n",
    "start_date = '2009-01-01'\n",
    "end_date = str(now.date())\n",
    "\n",
    "# меняем бесконечные значения на nan\n",
    "dataset_ff = dataset_ff.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# убираем пропуски в Debt_Equity (почему-то один вылезает постоянно)\n",
    "dataset_ff['Debt_Equity'] = dataset_ff['Debt_Equity'].replace(np.nan, max(dataset_ff['Debt_Equity']))    \n",
    "\n",
    "\n",
    "# заполняем пропуски в первых 22 колонках минимумами из этих колонок (до Debt/equity, не включая последний)\n",
    "for col in tqdm_notebook(list(dataset)[4:26]):\n",
    "    for i in range(len(dataset_ff)):\n",
    "        if dataset_ff[col][i] <> dataset_ff[col][i]:\n",
    "            dataset_ff[col][i] = np.min(dataset_ff[col].dropna().values)\n",
    "\n",
    "            \n",
    "print 'Исправляем ошибки в данных'\n",
    "\n",
    "with parallel_backend('threading', n_jobs = jobs_count):\n",
    "    Parallel()(delayed(fulfil)(m) for m in tqdm_notebook(range(len(dataset_ff))))\n",
    "\n",
    "# обнуляем отрицательное повышение дивов, иначе при подрезании сольётся весь ряд\n",
    "dataset_ff.loc[dataset_ff['div_growth_3yr'] < 0, 'div_growth_3yr'] = 0        \n",
    "dataset_ff.loc[dataset_ff['div_growth_5yr'] < 0, 'div_growth_5yr'] = 0        \n",
    "\n",
    "# если в div_inc_years пропуск, то эта компания не повышала дивиденды -> присваиваем значение нулю\n",
    "dataset_ff['div_inc_years'] = dataset_ff['div_inc_years'].replace(np.nan, 0)\n",
    "        \n",
    "# если байбек меньше -100, то меняем на -100\n",
    "dataset_ff.loc[dataset_ff['buyback'] < -100, 'buyback'] = -100\n",
    "        \n",
    "# отрицательный payout и debt/equity меняем на максимальный в колонках, т.к. этот столбец мы в итоге будем вычитать\n",
    "dataset_ff.loc[dataset_ff['payout'] < 0, 'payout'] = np.max(dataset_ff['payout'].dropna().values) \n",
    "dataset_ff.loc[dataset_ff['Debt_Equity'] < 0, 'Debt_Equity'] = np.max(dataset_ff['Debt_Equity'].dropna().values)        \n",
    "        \n",
    "# пропуски в payout и max_dd меняем на максмумы в колонках\n",
    "dataset_ff['max_dd'] = dataset_ff['max_dd'].replace(np.nan, np.max(dataset_ff['max_dd'].dropna().values) )\n",
    "dataset_ff['payout'] = dataset_ff['payout'].replace(np.nan, np.max(dataset_ff['payout'].dropna().values) )\n",
    " \n",
    "# пропуски в CAGR, sharpe и sortino меняем на минимумы\n",
    "dataset_ff['sortino'] = dataset_ff['sortino'].replace(np.nan, np.min(dataset_ff['sortino'].dropna().values) )\n",
    "dataset_ff['sharpe'] = dataset_ff['sharpe'].replace(np.nan, np.min(dataset_ff['sharpe'].dropna().values) )\n",
    "dataset_ff['CAGR'] = dataset_ff['CAGR'].replace(np.nan, np.min(dataset_ff['CAGR'].dropna().values) )\n",
    "    \n",
    "\n",
    "# если компания повышает дивы менее 5(3) лет, то и рост дивов за последние 5(3) лет должен быть равным 0        \n",
    "dataset_ff.loc[dataset_ff['div_inc_years'] < 3, 'div_growth_3yr'] = 0\n",
    "dataset_ff.loc[dataset_ff['div_inc_years'] < 5, 'div_growth_5yr'] = 0\n",
    "\n",
    "# Нулевые Quick_ratio меняем на Current_ratio\n",
    "dataset_ff.loc[dataset_ff['Quick_ratio'] == 0, 'Quick_ratio'] = dataset_ff['Current_ratio']\n",
    "\n",
    "# применяем функцию cut_over для наиболее проблемных столбцов, где максимальные значения являются неоправданными выбросами\n",
    "# после всей обработки обязательно снова взглянуть на данные визуально\n",
    "for col in list(dataset_ff)[4:38]:\n",
    "    if col == 'div_inc_years':\n",
    "        continue\n",
    "    if col == 'div_growth_3yr':\n",
    "        dataset_ff[col] = cut_over_less(dataset_ff[col])\n",
    "        continue\n",
    "    if col == 'div_growth_5yr':\n",
    "        dataset_ff[col] = cut_over_less(dataset_ff[col])\n",
    "        continue\n",
    "    if col == 'years_profitable':\n",
    "        continue\n",
    "    if col == 'years_up':\n",
    "        continue\n",
    "    if col == 'years_strike':\n",
    "        continue\n",
    "    else:\n",
    "        dataset_ff[col] = cut_over(dataset_ff[col])\n",
    "        dataset_ff[col] = cut_under(dataset_ff[col])\n",
    "        \n",
    "print 'Ошибки удалены, данные очищены'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем признаки, относящиеся к цене, их потом возьмём в отдельный датасет\n",
    "dataset_ff = dataset_ff.drop(dataset_ff.columns[[37, 38, 39, 40]], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок простой визуализации с целью обнаружения аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ff.boxplot(figsize=(2*dataset_ff.shape[1], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок парсинга данных для скидок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем историю цен\n",
      "[*********************100%***********************]  1268 of 1268 completed\n",
      "\n",
      "1 Failed download:\n",
      "- RDSA: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "print 'Собираем историю цен'\n",
    "\n",
    "yf.pdr_override() # <== that's all it takes :-)\n",
    "\n",
    "for i in range(len(tickers_only)):\n",
    "    if tickers_only[i] == 'BRK__A':\n",
    "        tickers_only[i] = 'BRK-A'\n",
    "    if tickers_only[i] == 'BRK__B':\n",
    "        tickers_only[i] = 'BRK-B'   \n",
    "    if tickers_only[i] == 'LEN__B':\n",
    "        tickers_only[i] = 'LEN-B'     \n",
    "    if tickers_only[i] == 'rdsa':\n",
    "        tickers_only[i] = 'RDS-A'\n",
    "    if tickers_only[i] == 'BF___B':\n",
    "        tickers_only[i] = 'BF-B'\n",
    "\n",
    "# download dataframe using pandas_datareader\n",
    "data_price_history = web.get_data_yahoo(list(tickers_only), start=\"2019-01-01\", end = end_date, auto_adjust = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_discount(i):\n",
    "    \n",
    "     \n",
    "    try:\n",
    "        table=pd.read_html('https://finance.yahoo.com/quote/' + tickers_only[i])\n",
    "        rg_table = table[1]\n",
    "    except:\n",
    "        p_e_current = 0\n",
    "        target_1yr = 0\n",
    "\n",
    "    try:\n",
    "        p_e_current = float(rg_table[1][2])\n",
    "    except:\n",
    "        p_e_current = 0\n",
    "    try:\n",
    "        target_1yr = float(rg_table[1][7])\n",
    "    except:\n",
    "        target_1yr = 0 \n",
    "\n",
    "    try:\n",
    "        table=pd.read_html('https://finance.yahoo.com/quote/' + tickers_only[i] +'/analysis?p=' + tickers_only[i])\n",
    "        rg_table = table[5]\n",
    "\n",
    "        next_5yr_growth = rg_table[tickers_only[i]][4]\n",
    "        str_finish = string.find(next_5yr_growth, '%')\n",
    "        next_5yr_growth = float(next_5yr_growth[:str_finish])\n",
    "    except:\n",
    "        next_5yr_growth = 0\n",
    "    try:    \n",
    "        table=pd.read_html('https://www.macrotrends.net/stocks/charts/' + tickers_only[i] + '/' + list_of_names[i]  + '/pe-ratio')\n",
    "        rg_table = table[0]\n",
    "        p_e_history = rg_table.iloc[:, 3].values\n",
    "        p_e_history = p_e_history[p_e_history != 0]\n",
    "        p_e_mean = np.median(p_e_history[:10])\n",
    "    except:\n",
    "        p_e_mean = 0\n",
    "    \n",
    "    try:\n",
    "        price_history =  data_price_history['Close'][data_price_history['Close'].columns[i]].values\n",
    "        m1 = np.histogram(price_history)\n",
    "        price_now = price_history[len(price_history)-1]\n",
    "        rem = m1[1][0]\n",
    "        stop = 0\n",
    "        price_cluster = list()\n",
    "        for k in range(len(m1[0])):\n",
    "            if m1[0][len(m1[0]) - k - 1] > 40:\n",
    "                price_cluster.append((m1[1][len(m1[0]) - k]))\n",
    "                stop = 1\n",
    "            else:\n",
    "                if stop == 1:\n",
    "                    price_cluster.append((m1[1][len(m1[0]) - k]))\n",
    "                    break\n",
    "        price_cluster_low = price_cluster[len(price_cluster) - 1]\n",
    "        price_cluster_high = price_cluster[0]\n",
    "        price_cluster_mid = (price_cluster_low + price_cluster_high) / 2\n",
    "        sale_cluster = 100 * (price_cluster_mid - price_now) / price_cluster_mid\n",
    "        \n",
    "    except:\n",
    "        sale_cluster = -100\n",
    "       \n",
    "    \n",
    "    dataset_discount.loc[i, 'p_e_current'] = p_e_current\n",
    "    dataset_discount.loc[i, 'target_1yr'] = target_1yr\n",
    "    dataset_discount.loc[i, 'next_5yr_growth'] = next_5yr_growth\n",
    "    dataset_discount.loc[i, 'p_e_mean'] = p_e_mean\n",
    "    dataset_discount.loc[i, 'sale_cluster'] = sale_cluster\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем таблицу скидок\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212b29d1d928480891cb8cf86154081b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1268), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 19min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print 'Собираем таблицу скидок'\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset_discount = dataset_ff[list(dataset_ff)[0:4]]\n",
    "dataset_discount['div_yield_5yr'] = dataset['div_yield_5yr']\n",
    "dataset_discount['div_inc_years'] = dataset_ff['div_inc_years']\n",
    "dataset_discount['curr_stock_price'] = dataset['curr_stock_price']\n",
    "dataset_discount['52_week_high'] = dataset['52_week_high']\n",
    "dataset_discount['52_week_low'] = dataset['52_week_low']\n",
    "dataset_discount['eps_growth_5y'] = dataset_ff['eps_growth_5y']\n",
    "\n",
    "\n",
    "with parallel_backend('threading', n_jobs = jobs_count):\n",
    "    Parallel()(delayed(parsing_discount)(x) for x in tqdm_notebook(range(len(dataset_discount))))\n",
    "\n",
    "\n",
    "dataset_discount['TTM_div_yield'] = dataset_ff['TTM_div_yield']\n",
    "dataset_discount = dataset_discount.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок расчёта баллов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Переводим показатели в баллы\n"
     ]
    }
   ],
   "source": [
    "print 'Переводим показатели в баллы'\n",
    "\n",
    "# забираем dataset_ff\n",
    "dataset_score_calc = copy.deepcopy(dataset_ff)\n",
    "dataset_score_calc = dataset_score_calc[dataset_score_calc.columns[:37]]\n",
    "# Теперь считаем баллы\n",
    "# Цель - сформировать датасет со значениями от 0 до 10, где 10 - лучший показатель, 0 - худший\n",
    "# основная логика - отрицательные значения делаем нулевыми, слишком большие значения (большие медианы + 3 стд откл) делаем 10\n",
    "# остальные значения берём через арктангенс для сглаживания (чтобы прирост больших значений был менее значим, чем прирост небольших)\n",
    "\n",
    "dataset_score_calc['revenue_growth_10y'] = dataset_ff['revenue_growth_10y'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['revenue_growth_10y'] = dataset_score_calc['revenue_growth_10y'].map(lambda x: 10 if x > max_value(dataset_score_calc['revenue_growth_10y']) else (10 * np.arctan(x/3) / np.arctan(max_value(dataset_score_calc['revenue_growth_10y']))))\n",
    "dataset_score_calc['revenue_growth_5y'] = dataset_ff['revenue_growth_5y'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['revenue_growth_5y'] = dataset_score_calc['revenue_growth_5y'].map(lambda x: 10 if x > max_value(dataset_ff['revenue_growth_5y']) else (10 * np.arctan(x/3) / np.arctan(max_value(dataset_ff['revenue_growth_5y']))))\n",
    "dataset_score_calc['revenue_coef'] = dataset_ff['revenue_coef'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['revenue_coef'] = dataset_score_calc['revenue_coef'].map(lambda x: 10 if x > max_value(dataset_ff['revenue_coef']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['revenue_coef']))))\n",
    "dataset_score_calc['eps_growth_10y'] = dataset_ff['eps_growth_10y'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['eps_growth_10y'] = dataset_score_calc['eps_growth_10y'].map(lambda x: 10 if x > max_value(dataset_ff['eps_growth_10y']) else (10 * np.arctan(x/3) / np.arctan(max_value(dataset_ff['eps_growth_10y']))))\n",
    "dataset_score_calc['eps_growth_5y'] = dataset_ff['eps_growth_5y'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['eps_growth_5y'] = dataset_score_calc['eps_growth_5y'].map(lambda x: 10 if x > max_value(dataset_ff['eps_growth_5y']) else (10 * np.arctan(x/3) / np.arctan(max_value(dataset_ff['eps_growth_5y']))))\n",
    "dataset_score_calc['next_5yr_growth'] = dataset_ff['next_5yr_growth'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['next_5yr_growth'] = dataset_score_calc['next_5yr_growth'].map(lambda x: 10 if x > max_value(dataset_ff['next_5yr_growth']) else (10 * np.arctan(x/3) / np.arctan(max_value(dataset_ff['next_5yr_growth']))))\n",
    "dataset_score_calc['eps_coef'] = dataset_ff['eps_coef'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['eps_coef'] = dataset_score_calc['eps_coef'].map(lambda x: 10 if x > max_value(dataset_ff['eps_coef']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['eps_coef']))))\n",
    "dataset_score_calc['R_square_10y'] = 10 * dataset_ff['R_square_10y']\n",
    "dataset_score_calc['years_profitable'] = 10 * dataset_ff['years_profitable'] / max(dataset_ff['years_profitable'])\n",
    "dataset_score_calc['years_up'] = 10 * dataset_ff['years_up'] / max(dataset_ff['years_up'])\n",
    "dataset_score_calc['years_strike'] = 10 * dataset_ff['years_strike'] / max(dataset_ff['years_strike'])\n",
    "dataset_score_calc['market_cap'] = dataset_ff['market_cap'].map(lambda x: 10 if x > 200 else (x / 20))\n",
    "dataset_score_calc['buyback'] = dataset_ff['buyback'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['buyback'] = dataset_score_calc['buyback'].map(lambda x: 10 if x > 35 else x / 3.5)\n",
    "dataset_score_calc['ROE'] = dataset_ff['ROE'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['ROE'] = dataset_score_calc['ROE'].map(lambda x: 10 if x > max_value(dataset_ff['ROE']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['ROE']))))\n",
    "dataset_score_calc['mean_ROE'] = dataset_ff['mean_ROE'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['mean_ROE'] = dataset_score_calc['mean_ROE'].map(lambda x: 10 if x > max_value(dataset_ff['mean_ROE']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['mean_ROE']))))\n",
    "dataset_score_calc['ROA'] = dataset_ff['ROA'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['ROA'] = dataset_score_calc['ROA'].map(lambda x: 10 if x > max_value(dataset_ff['ROA']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['ROA']))))\n",
    "dataset_score_calc['mean_ROA'] = dataset_ff['mean_ROA'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['mean_ROA'] = dataset_score_calc['mean_ROA'].map(lambda x: 10 if x > max_value(dataset_ff['mean_ROA']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['mean_ROA']))))\n",
    "dataset_score_calc['Shareholder_Equity_growth'] = dataset_ff['Shareholder_Equity_growth'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['Shareholder_Equity_growth'] = dataset_score_calc['Shareholder_Equity_growth'].map(lambda x: 10 if x > 10 else x)\n",
    "dataset_score_calc['Profit_margin'] = dataset_ff['Profit_margin'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['Profit_margin'] = dataset_score_calc['Profit_margin'].map(lambda x: 10 if x > max_value(dataset_ff['Profit_margin']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['Profit_margin']))))\n",
    "dataset_score_calc['Profit_margin_mean'] = dataset_ff['Profit_margin_mean'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['Profit_margin_mean'] = dataset_score_calc['Profit_margin_mean'].map(lambda x: 10 if x > max_value(dataset_ff['Profit_margin_mean']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['Profit_margin_mean']))))\n",
    "dataset_score_calc['Quick_ratio'] = dataset_score_calc['Quick_ratio'].map(lambda x: 10 if x > 1 else (10 * np.arctan(x) / np.arctan(1) ))\n",
    "dataset_score_calc['Current_ratio'] = dataset_score_calc['Current_ratio'].map(lambda x: 10 if x > 1 else (10 * np.arctan(x) / np.arctan(1) ))\n",
    "dataset_score_calc['Debt_Equity'] = dataset_ff['Debt_Equity'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['Debt_Equity'] = dataset_score_calc['Debt_Equity'].map(lambda x: 10 if np.arctan(x) > 1 else np.arctan(x))\n",
    "dataset_score_calc['Debt_Equity'] = 10 - dataset_score_calc['Debt_Equity'] \n",
    "dataset_score_calc['payout'] = dataset_score_calc['payout'].map(lambda x: 10 if x > max_value(dataset_ff['payout']) else (10 * x / max_value(dataset_ff['payout'])))\n",
    "dataset_score_calc['payout'] = 10 - dataset_score_calc['payout'] \n",
    "dataset_score_calc['payout_5yr'] = dataset_score_calc['payout_5yr'].map(lambda x: 10 if x > max_value(dataset_ff['payout_5yr']) else (10 * x / max_value(dataset_ff['payout_5yr'])))\n",
    "dataset_score_calc['payout_5yr'] = 10 - dataset_score_calc['payout_5yr'] \n",
    "dataset_score_calc['CAGR'] = dataset_ff['CAGR'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['CAGR'] = dataset_score_calc['CAGR'].map(lambda x: 10 if x > 30 else x/3 )\n",
    "dataset_score_calc['max_dd'] = dataset_ff['max_dd'].map(lambda x: 100 if x == 0 else x)\n",
    "dataset_score_calc['max_dd'] = dataset_score_calc['max_dd'].map(lambda x: 10 if x > 90 else (x - min(dataset_score_calc['max_dd']) )/ 9)\n",
    "dataset_score_calc['max_dd'] = 10 - dataset_score_calc['max_dd'] \n",
    "dataset_score_calc['sharpe'] = dataset_ff['sharpe'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['sharpe'] = dataset_score_calc['sharpe'].map(lambda x: 10 if x > max_value(dataset_ff['sharpe']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['sharpe']))))\n",
    "dataset_score_calc['sortino'] = dataset_ff['sortino'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['sortino'] = dataset_score_calc['sortino'].map(lambda x: 10 if x > max_value(dataset_ff['sortino']) else (10 * np.arctan(x) / np.arctan(max_value(dataset_ff['sortino']))))\n",
    "dataset_score_calc['TTM_div_yield'] = dataset_ff['TTM_div_yield'].map(lambda x: 0 if x < 0 else x)\n",
    "dataset_score_calc['TTM_div_yield'] = dataset_score_calc['TTM_div_yield'].map(lambda x: 10 if x > 8 else x/0.8 )\n",
    "dataset_score_calc['div_inc_years'] = dataset_ff['div_inc_years'].map(lambda x: 10 if x > 50 else x / 5)\n",
    "dataset_score_calc['div_growth_3yr'] = dataset_ff['div_growth_3yr'].map(lambda x: 10 if x > 30 else x / 3)\n",
    "dataset_score_calc['div_growth_5yr'] = dataset_ff['div_growth_5yr'].map(lambda x: 10 if x > 30 else x / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок расчёта общего рейтинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Считаем рейтинг\n"
     ]
    }
   ],
   "source": [
    "print 'Считаем рейтинг'\n",
    "\n",
    "dataset_rating  = dataset_score_calc[dataset_score_calc.columns[:4]]\n",
    "dataset_rating['Growth_score'] = map(int, map(round, dataset_score_calc[dataset_score_calc.columns[[4, 5, 6, 7, 8, 8, 8, 9, 9, 9, 10, 11, 13, 14, 22, 23, 29, 29, 32, 32]]].sum(axis=1) / (len(dataset_score_calc.columns[[4, 5, 6, 7, 8, 8, 8, 9, 9, 9, 10, 11, 13, 14, 22, 23, 29, 29, 32, 32]]) / 10.)))\n",
    "dataset_rating['Dividend_score'] = map(int, map(round, dataset_score_calc[dataset_score_calc.columns[[27, 28, 33, 33, 33, 33, 34, 34, 34, 35, 36]]].sum(axis=1) / (len(dataset_score_calc.columns[[27, 28, 33, 33, 33, 33, 34, 34, 34, 35, 36]]) / 10.)))\n",
    "dataset_rating['Stability_score'] = map(int, map(round, dataset_score_calc[dataset_score_calc.columns[[10, 11, 12, 12, 14, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27, 30, 30, 30, 30,  31, 31, 31, 32, 32, 32, 34, 34, 34, 34, 34]]].sum(axis=1) / (len(dataset_score_calc.columns[[10, 11, 12, 12, 14, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27, 30, 30, 30, 30,  31, 31, 31, 32, 32, 32, 34, 34, 34, 34, 34]]) / 10.)))\n",
    "dataset_rating['Overall_score'] = map(int, map(round, dataset_score_calc.sum(axis=1) / 3.3))\n",
    "dataset_rating['Ready_to_boost'] = map(int, map(round, dataset_score_calc[dataset_score_calc.columns[[4, 6, 6, 9]]].sum(axis=1) / (len(dataset_score_calc.columns[[4, 6, 6, 9]]) / 4.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок расчёта скидок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Считаем скидки\n"
     ]
    }
   ],
   "source": [
    "print 'Считаем скидки'\n",
    "\n",
    "dataset_discount.loc[dataset_discount['div_inc_years'] < 5, 'div_yield_5yr'] = 0\n",
    "discount_calc = copy.deepcopy(dataset_discount)\n",
    "discount_calc = discount_calc[discount_calc.columns[:4]]\n",
    "discount_calc['dividend_discount'] = 100 * (dataset_discount['TTM_div_yield'] - dataset_discount['div_yield_5yr']) / dataset_discount['TTM_div_yield']\n",
    "discount_calc.loc[dataset_discount['div_yield_5yr'] == 0, 'dividend_discount'] = None\n",
    "discount_calc['p_e_discount'] = 100 * (dataset_discount['p_e_mean'] - dataset_discount['p_e_current']) / dataset_discount['p_e_mean']\n",
    "discount_calc.loc[dataset_discount['p_e_current'] == 0, 'p_e_discount'] = None\n",
    "discount_calc['target_discount'] = 100 * (dataset_discount['target_1yr'] - dataset_discount['curr_stock_price']) / dataset_discount['target_1yr']\n",
    "discount_calc['paypack_past'] = np.log(dataset_discount['p_e_current'] * (dataset_discount['eps_growth_5y']/100.) + 1) / np.log(1 + dataset_discount['eps_growth_5y']/100.)\n",
    "discount_calc.loc[discount_calc['paypack_past'] == 0, 'paypack_past'] = None\n",
    "discount_calc['paypack_future'] = np.log(dataset_discount['p_e_current'] * (dataset_discount['next_5yr_growth']/100.) + 1) / np.log(1 + dataset_discount['next_5yr_growth']/100.)\n",
    "discount_calc.loc[discount_calc['paypack_future'] == 0, 'paypack_future'] = None\n",
    "discount_calc['high_low_discount'] = 100 * ((dataset_discount['52_week_high'] + dataset_discount['52_week_low']) / 2. - dataset_discount['curr_stock_price']) / ((dataset_discount['52_week_high'] + dataset_discount['52_week_low']) / 2.)\n",
    "discount_calc['sale_cluster'] = dataset_discount['sale_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блок отображения покупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собираем таблицы с покупками\n"
     ]
    }
   ],
   "source": [
    "print 'Собираем таблицы с покупками'\n",
    "\n",
    "to_buy_calc = dataset_rating.merge(discount_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны чисто растущие компании\n",
    "to_buy_calc_growth = copy.deepcopy(to_buy_calc)\n",
    "full_growth = to_buy_calc_growth.sort_values(by = 'Growth_score', ascending = False).reset_index(drop=True)\n",
    "del to_buy_calc_growth['dividend_discount']\n",
    "to_buy_calc_growth = to_buy_calc_growth.sort_values(by = 'Growth_score', ascending = False)[:50].reset_index(drop=True)\n",
    "to_buy_calc_growth.loc[to_buy_calc_growth['p_e_discount'] < -5, 'p_e_discount'] = None\n",
    "to_buy_calc_growth.loc[to_buy_calc_growth['target_discount'] < 0, 'target_discount'] = None\n",
    "to_buy_calc_growth.loc[to_buy_calc_growth['high_low_discount'] < 0, 'high_low_discount'] = None\n",
    "to_buy_calc_growth.loc[to_buy_calc_growth['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_calc_growth.loc[to_buy_calc_growth['paypack_past'] > 13, 'paypack_past'] = None\n",
    "to_buy_calc_growth.loc[to_buy_calc_growth['paypack_future'] > 13, 'paypack_future'] = None\n",
    "to_buy_calc_growth = to_buy_calc_growth.dropna().sort_values(by = 'Growth_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны чисто стабильные компании\n",
    "to_buy_calc_stable = copy.deepcopy(to_buy_calc)\n",
    "del to_buy_calc_stable['dividend_discount']\n",
    "full_stable = to_buy_calc_stable.sort_values(by = 'Stability_score', ascending = False).reset_index(drop=True)\n",
    "to_buy_calc_stable = to_buy_calc_stable.sort_values(by = 'Stability_score', ascending = False)[:50].reset_index(drop=True)\n",
    "to_buy_calc_stable.loc[to_buy_calc_stable['p_e_discount'] < -5, 'p_e_discount'] = None\n",
    "to_buy_calc_stable.loc[to_buy_calc_stable['target_discount'] < -5, 'target_discount'] = None\n",
    "to_buy_calc_stable.loc[to_buy_calc_stable['high_low_discount'] < -5, 'high_low_discount'] = None\n",
    "to_buy_calc_stable.loc[to_buy_calc_stable['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_calc_stable.loc[to_buy_calc_stable['paypack_past'] > 15, 'paypack_past'] = None\n",
    "to_buy_calc_stable.loc[to_buy_calc_stable['paypack_future'] > 15, 'paypack_future'] = None\n",
    "to_buy_calc_stable = to_buy_calc_stable.dropna().sort_values(by = 'Stability_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны растущие стабильные компании\n",
    "to_buy_calc_growth_stable = copy.deepcopy(to_buy_calc)\n",
    "del to_buy_calc_growth_stable['dividend_discount']\n",
    "to_buy_calc_growth_stable['growth_stable'] = (to_buy_calc_growth_stable['Growth_score']-np.mean(to_buy_calc_growth_stable['Growth_score']))/(np.std(to_buy_calc_growth_stable['Growth_score'])) + (to_buy_calc_growth_stable['Stability_score']-np.mean(to_buy_calc_growth_stable['Stability_score']))/ (np.std(to_buy_calc_growth_stable['Stability_score']))\n",
    "full_growth_stable = to_buy_calc_growth_stable.sort_values(by = 'growth_stable', ascending = False).reset_index(drop=True)\n",
    "to_buy_calc_growth_stable = to_buy_calc_growth_stable.sort_values(by = 'growth_stable', ascending = False)[:50].reset_index(drop=True)\n",
    "to_buy_calc_growth_stable.loc[to_buy_calc_growth_stable['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_calc_growth_stable.loc[to_buy_calc_growth_stable['p_e_discount'] < -5, 'p_e_discount'] = None\n",
    "to_buy_calc_growth_stable.loc[to_buy_calc_growth_stable['target_discount'] < -5, 'target_discount'] = None\n",
    "to_buy_calc_growth_stable.loc[to_buy_calc_growth_stable['high_low_discount'] < -5, 'high_low_discount'] = None\n",
    "to_buy_calc_growth_stable.loc[to_buy_calc_growth_stable['paypack_past'] > 15, 'paypack_past'] = None\n",
    "to_buy_calc_growth_stable.loc[to_buy_calc_growth_stable['paypack_future'] > 15, 'paypack_future'] = None\n",
    "to_buy_calc_growth_stable = to_buy_calc_growth_stable.dropna().sort_values(by = 'growth_stable', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны растущие дивидендные компании\n",
    "\n",
    "to_buy_growth_dividend = copy.deepcopy(to_buy_calc)\n",
    "to_buy_growth_dividend['growth_dividend'] = (to_buy_growth_dividend['Growth_score']-np.mean(to_buy_growth_dividend['Growth_score']))/(np.std(to_buy_growth_dividend['Growth_score'])) + (to_buy_growth_dividend['Dividend_score']-np.mean(to_buy_growth_dividend['Dividend_score']))/ (np.std(to_buy_growth_dividend['Dividend_score']))\n",
    "full_growth_dividend = to_buy_growth_dividend.sort_values(by = 'growth_dividend', ascending = False).reset_index(drop=True)\n",
    "to_buy_growth_dividend = to_buy_growth_dividend.sort_values(by = 'growth_dividend', ascending = False)[:100].reset_index(drop=True)\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['p_e_discount'] < 0, 'p_e_discount'] = None\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['dividend_discount'] < 0, 'dividend_discount'] = None\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['target_discount'] < 0, 'target_discount'] = None\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['high_low_discount'] < 0, 'high_low_discount'] = None\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['paypack_past'] > 10, 'paypack_past'] = None\n",
    "to_buy_growth_dividend.loc[to_buy_growth_dividend['paypack_future'] > 10, 'paypack_future'] = None\n",
    "to_buy_growth_dividend = to_buy_growth_dividend.dropna().sort_values(by = 'growth_dividend', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны сбалансированные компании\n",
    "to_buy_calc_balanced = copy.deepcopy(to_buy_calc)\n",
    "full_balanced = to_buy_calc_balanced.sort_values(by = 'Overall_score', ascending = False).reset_index(drop=True)\n",
    "del to_buy_calc_balanced['dividend_discount']\n",
    "to_buy_calc_balanced = to_buy_calc_balanced.sort_values(by = 'Overall_score', ascending = False)[:100].reset_index(drop=True)\n",
    "to_buy_calc_balanced.loc[to_buy_calc_balanced['p_e_discount'] < -5, 'p_e_discount'] = None\n",
    "to_buy_calc_balanced.loc[to_buy_calc_balanced['target_discount'] < -5, 'target_discount'] = None\n",
    "to_buy_calc_balanced.loc[to_buy_calc_balanced['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_calc_balanced.loc[to_buy_calc_balanced['high_low_discount'] < -5, 'high_low_discount'] = None\n",
    "to_buy_calc_balanced.loc[to_buy_calc_balanced['paypack_past'] > 13, 'paypack_past'] = None\n",
    "to_buy_calc_balanced.loc[to_buy_calc_balanced['paypack_future'] > 13, 'paypack_future'] = None\n",
    "to_buy_calc_balanced = to_buy_calc_balanced.dropna().sort_values(by = 'Overall_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны дивидендные компании\n",
    "to_buy_dividend = copy.deepcopy(to_buy_calc)\n",
    "full_dividend = to_buy_dividend.sort_values(by = 'Dividend_score', ascending = False).reset_index(drop=True)\n",
    "del to_buy_dividend['Growth_score']\n",
    "del to_buy_dividend['Stability_score']\n",
    "del to_buy_dividend['Overall_score']\n",
    "del to_buy_dividend['p_e_discount']\n",
    "to_buy_dividend['Dividend_yield'] = dataset_ff['TTM_div_yield']\n",
    "to_buy_dividend = to_buy_dividend.sort_values(by = 'Dividend_score', ascending = False)[:20].reset_index(drop=True)\n",
    "to_buy_dividend.loc[to_buy_dividend['dividend_discount'] < 0, 'dividend_discount'] = None\n",
    "to_buy_dividend.loc[to_buy_dividend['high_low_discount'] < 0, 'high_low_discount'] = None\n",
    "to_buy_dividend.loc[to_buy_dividend['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_dividend.loc[to_buy_dividend['target_discount'] < 0, 'target_discount'] = None\n",
    "to_buy_dividend.loc[to_buy_dividend['paypack_past'] > 13, 'paypack_past'] = None\n",
    "to_buy_dividend.loc[to_buy_dividend['paypack_future'] > 13, 'paypack_future'] = None\n",
    "to_buy_dividend = to_buy_dividend.dropna().sort_values(by = 'Dividend_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны дивидендные стабильные компании\n",
    "to_buy_dividend_stable = copy.deepcopy(to_buy_calc)\n",
    "to_buy_dividend_stable['Dividend_yield'] = dataset_ff['TTM_div_yield']\n",
    "to_buy_dividend_stable['dividend_stable'] = (to_buy_dividend_stable['Dividend_score']-np.mean(to_buy_dividend_stable['Dividend_score']))/(np.std(to_buy_dividend_stable['Dividend_score'])) + (to_buy_dividend_stable['Stability_score']-np.mean(to_buy_dividend_stable['Stability_score']))/ (np.std(to_buy_dividend_stable['Stability_score']))\n",
    "full_dividend_stable = to_buy_dividend_stable.sort_values(by = 'dividend_stable', ascending = False).reset_index(drop=True)\n",
    "del to_buy_dividend_stable['p_e_discount']\n",
    "del to_buy_dividend_stable['target_discount']\n",
    "del to_buy_dividend_stable['Growth_score']\n",
    "del to_buy_dividend_stable['Overall_score']\n",
    "del to_buy_dividend_stable['paypack_past']\n",
    "del to_buy_dividend_stable['paypack_future']\n",
    "to_buy_dividend_stable = to_buy_dividend_stable.sort_values(by = 'dividend_stable', ascending = False)[:20].reset_index(drop=True)\n",
    "to_buy_dividend_stable.loc[to_buy_dividend_stable['dividend_discount'] < 0, 'dividend_discount'] = None\n",
    "to_buy_dividend_stable.loc[to_buy_dividend_stable['sale_cluster'] < 0, 'sale_cluster'] = None\n",
    "to_buy_dividend_stable.loc[to_buy_dividend_stable['high_low_discount'] < 0, 'high_low_discount'] = None\n",
    "to_buy_dividend_stable.loc[to_buy_dividend_stable['Dividend_yield'] < dataset_ff[dataset_ff['TTM_div_yield'] != 0]['TTM_div_yield'].mean(), 'Dividend_yield'] = None\n",
    "to_buy_dividend_stable = to_buy_dividend_stable.dropna().sort_values(by = 'dividend_stable', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужны компании с потенциальным взрывом роста\n",
    "to_buy_boost = copy.deepcopy(to_buy_calc)\n",
    "full_boost = to_buy_boost.sort_values(by = 'Ready_to_boost', ascending = False).reset_index(drop=True)\n",
    "del to_buy_boost['p_e_discount']\n",
    "del to_buy_boost['Growth_score']\n",
    "del to_buy_boost['Overall_score']\n",
    "to_buy_boost = to_buy_boost.sort_values(by = 'Ready_to_boost', ascending = False)[:50].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter('stocks_to_buy.xlsx') as writer:  \n",
    "    to_buy_calc_growth.to_excel(writer, sheet_name='growth')\n",
    "    to_buy_calc_stable.to_excel(writer, sheet_name='stable')\n",
    "    to_buy_dividend.to_excel(writer, sheet_name='dividend')\n",
    "    to_buy_calc_growth_stable.to_excel(writer, sheet_name='growth_and_stable')\n",
    "    to_buy_dividend_stable.to_excel(writer, sheet_name='dividend_and_stable')\n",
    "    to_buy_growth_dividend.to_excel(writer, sheet_name='growth_and_dividend')\n",
    "    to_buy_calc_balanced.to_excel(writer, sheet_name='balanced')   \n",
    "    to_buy_boost.to_excel(writer, sheet_name='ready_to_boost')   \n",
    "    \n",
    "\n",
    "with pd.ExcelWriter('all_stocks.xlsx') as writer:  \n",
    "    full_growth.to_excel(writer, sheet_name='growth')\n",
    "    full_stable.to_excel(writer, sheet_name='stable')\n",
    "    full_dividend.to_excel(writer, sheet_name='dividend')\n",
    "    full_growth_stable.to_excel(writer, sheet_name='growth_and_stable')\n",
    "    full_dividend_stable.to_excel(writer, sheet_name='dividend_and_stable')\n",
    "    full_growth_dividend.to_excel(writer, sheet_name='growth_and_dividend')\n",
    "    full_balanced.to_excel(writer, sheet_name='balanced')      \n",
    "    full_boost.to_excel(writer, sheet_name='ready_to_boost')   \n",
    "    \n",
    "print 'Готово!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
